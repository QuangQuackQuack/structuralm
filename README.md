
# Text-to-3D Scene Pipeline (Multi-Agent â†’ GAT â†’ DreamFusion â†’ Composition)

This repo contains an end-to-end pipeline that turns a **text prompt** into a **3D scene**:

1. A **multi-agent LLM system** builds a structured scene graph.
2. A **Graph Attention Network (GAT)** predicts 3D positions & rotations.
3. **DreamFusion (DeepFloyd IF)** generates per-object meshes.
4. A **composition module** assembles everything into a single `.obj + .mtl` scene.

âš ï¸ **Status**: research / prototype code. Expect to tweak configs and paths for your own setup.

---

## 1. Requirements

Tested with:

- **Python**: 3.11  
- **CUDA**: 12.1  
- **PyTorch**: `2.4.1+cu121`  
- **GPU**: NVIDIA GPU with at least **8â€“12 GB VRAM** (more is better; DreamFusion is heavy)

You will also need:

- An LLM endpoint compatible with `langchain-openai` (for the multi-agent system)
- A Hugging Face account + token (for downloading DeepFloyd IF weights)

---

## 2. Installation

From the repo root:

```bash
# 1) (Recommended) create & activate a virtualenv / conda env
# conda create -n construct2 python=3.11
# conda activate construct2

# 2) Install base dependencies
pip install -r requirements.txt

# 3) Install extra packages that require a proper Git build context
pip install -r required_git.txt --no-build-isolation
````

> ðŸ’¡ Make sure your PyTorch build matches your CUDA version
> (e.g. `torch==2.4.1+cu121` for CUDA 12.1).

---

## 3. Configuration

### 3.1. Create your run script

This repo tracks a **template** shell script:

* `run_template.sh` â€” safe, no keys inside

Do **not** edit this directly. Instead:

```bash
cp run_template.sh run_pipeline.sh
chmod +x run_pipeline.sh
```

Then open `run_pipeline.sh` and fill in your own values:

```bash
export MY_LLM_API_KEY="sk-..."                 # your LLM API key
export MY_LLM_BASE_URL="https://provider/"     # your LLM base URL
export SCENE_MODEL_NAME="Llama-3.3-70B-Instruct"  # or your model name
export MY_HUGGINGFACE_TOKEN="hf_..."           # HF token for DeepFloyd IF
```

> `run_pipeline.sh` is **git-ignored** so your secrets wonâ€™t be committed.

---

### 3.2. Set the input prompt

The main entrypoint is:

```text
src/complete_pipeline/main.py
```

Inside, there is a hard-coded test prompt (for now).
Edit it to whatever scene you want, for example:

```python
user_prompt = (
    "An astronaut sitting on a red sofa in a cozy living room, "
    "with a small coffee table in front of the sofa."
)
```

Later you can wire this to a CLI arg or UI; for now itâ€™s simple and explicit.

---

## 4. Running the Pipeline

From the repo root:

```bash
./run_pipeline.sh
```

The script will:

1. Export env vars (LLM + Hugging Face).

2. Run the pipeline with:

   ```bash
   PYTHONPATH=src:. python -m complete_pipeline.main
   ```

3. Print a **run_id** and a **total elapsed time** at the end, e.g.:

   ```text
   [PIPELINE] run_id = 2025-11-30_22-55-33
   [PIPELINE] run_dir = /.../complete_pipeline/temp/2025-11-30_22-55-33
   [PIPELINE] Total elapsed time: 00:23:41 (exit code 0)
   ```

---

## 5. Outputs & Folder Structure (Quick Overview)

For each run, all intermediate artifacts go into:

```text
temp/<run_id>/
```

Rough layout:

* `temp/<run_id>/multi_agents/`

  * `scene_graph.json` â€“ LLM-generated scene graph
  * `pipeline_scene.json` â€“ structured internal scene representation
  * `traces.json` â€“ per-agent debugging traces
* `temp/<run_id>/graph_attention_network/`

  * `scene_for_gat.json` â€“ input graph for GAT
  * `gat_output.json` â€“ predicted positions & rotations
  * `pipeline_scene_after_gat.json` â€“ scene updated with GAT results
* `temp/<run_id>/dreamfusion/`

  * `<object_label>_<index>.obj/.mtl` â€“ meshes generated by DreamFusion
* `temp/<run_id>/compositions/`

  * `pos.json` â€“ composition input (bounding boxes, centers, rotations)
  * `scene_001.obj` / `scene_001.mtl` â€“ **final composed scene**

That final `scene_001.obj` + `scene_001.mtl` is what you import into Blender, Unity, etc.

---

## 6. Pipeline Stages (Very Short Summary)

You can fill this out more later; for now:

1. **Multi-Agent Scene Graph (LangGraph + LLM)**

   * Enriches the user prompt.
   * Extracts objects and relationships.
   * Estimates real-world sizes and room dimensions.
   * Produces a normalized scene graph.

2. **Graph Attention Network (GAT) Layout Module**

   * Takes the scene graph as a PyTorch Geometric graph.
   * Predicts each objectâ€™s 3D position & rotation.
   * Writes predictions into a JSON (`gat_output.json`).

3. **DreamFusion (DeepFloyd IF-based)**

   * For each object label + prompt, trains a NeRF-style implicit model.
   * Exports a textured mesh (`model.obj/.mtl`) per object.
   * Pipeline copies these into `temp/<run_id>/dreamfusion`.

4. **Composition & Export**

   * Reads GAT predictions + DreamFusion meshes.
   * Scales, rotates, and places each object.
   * Applies simple gravity/stacking so objects rest on a common floor.
   * Exports a single unified **scene OBJ+MTL**.

---

## 7. Notes

* Training DreamFusion is **slow** and GPU-intensive. If you only want to test the rest of the pipeline, you can temporarily disable the DreamFusion step (see `main_without_dream.py` or comment that node).
* The code is organized to keep:

  * reusable modules in `src/complete_pipeline/`
  * external components in their own folders (`Multi_Agents/`, `Graph_Attention_Network/`, `DreamFusion/`, `Composition/`)
